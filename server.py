"""
FastAPIåç«¯æœåŠ¡
ä¸ºéœ€æ±‚æ¾„æ¸…åŠ©æ‰‹æä¾›RESTful APIæ¥å£å’ŒWebSocketå®æ—¶é€šä¿¡
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import os
import asyncio
from datetime import datetime
from dotenv import load_dotenv
import socketio

from langchain_openai import ChatOpenAI
from search import search_requirement_context

load_dotenv()

# FastAPIåº”ç”¨
app = FastAPI(
    title="éœ€æ±‚æ¾„æ¸…åŠ©æ‰‹API",
    description="AIéœ€æ±‚æ¾„æ¸…åŠ©æ‰‹çš„RESTful APIæ¥å£",
    version="1.0.0"
)

# Socket.IOè®¾ç½®
sio = socketio.AsyncServer(async_mode='asgi', cors_allowed_origins='*')
socket_app = socketio.ASGIApp(sio, app)

# CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:8501", "http://localhost:8504", "http://127.0.0.1:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ•°æ®æ¨¡å‹
class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    message: str
    conversation_history: List[ChatMessage] = []
    enable_search: bool = True

class ChatResponse(BaseModel):
    response: str
    timestamp: str
    search_info: Optional[str] = None
    conversation_id: str

class RequirementAnalysis(BaseModel):
    original_requirement: str
    optimized_requirement: str
    key_questions: List[Dict[str, str]]
    suggestions: List[str]

# å…¨å±€å˜é‡
llm = ChatOpenAI(
    model="deepseek-chat",
    openai_api_key=os.getenv("DEEPSEEK_API_KEY"),
    openai_api_base=os.getenv("DEEPSEEK_BASE_URL"),
    streaming=False
)

# å†…å­˜å­˜å‚¨ï¼ˆç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨æ•°æ®åº“ï¼‰
conversations: Dict[str, List[ChatMessage]] = {}

def generate_conversation_id() -> str:
    """ç”Ÿæˆå¯¹è¯ID"""
    return f"conv_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

async def save_conversation(conversation_id: str, user_message: str, ai_response: str):
    """ä¿å­˜å¯¹è¯å†å²"""
    if conversation_id not in conversations:
        conversations[conversation_id] = []

    conversations[conversation_id].extend([
        ChatMessage(role="user", content=user_message),
        ChatMessage(role="assistant", content=ai_response)
    ])

# Socket.IO äº‹ä»¶å¤„ç†
@sio.event
async def connect(sid, environ):
    print(f"Client connected: {sid}")

@sio.event
async def disconnect(sid):
    print(f"Client disconnected: {sid}")

@sio.event
async def chat_message(sid, data):
    """å¤„ç†èŠå¤©æ¶ˆæ¯å¹¶æµå¼è¿”å›"""
    try:
        message = data.get('message')
        history_data = data.get('history', [])
        enable_search = data.get('enable_search', True)
        conversation_id = data.get('conversation_id') or generate_conversation_id()
        
        # è½¬æ¢å†å²è®°å½•
        history = [{"role": msg['role'], "content": msg['content']} for msg in history_data]
        
        # æœç´¢ä¿¡æ¯
        search_info = ""
        if (enable_search and
            len(history) == 0 and
            len(message) > 10 and
            os.getenv("SERPER_API_KEY")):
            try:
                await sio.emit('search_status', {'status': 'searching'}, room=sid)
                # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥æœç´¢å‡½æ•°
                search_info = await asyncio.to_thread(search_requirement_context, message)
                await sio.emit('search_status', {'status': 'completed', 'info': search_info}, room=sid)
            except Exception as e:
                print(f"Search error: {e}")
                await sio.emit('search_status', {'status': 'error', 'error': str(e)}, room=sid)

        # æ„å»ºæç¤ºè¯
        prompt = f"""### SYSTEM CONTEXT
**C (Context)**: You are an expert-level Prompt Engineer and Requirements Analyst with deep expertise in AI interaction design, software development methodologies, and systematic thinking frameworks. Your audience is users who need professional-grade prompts for complex tasks. Your goal is to transform vague requirements into precise, actionable, and highly effective prompts.

**T (Task)**: Analyze, deconstruct, and systematically clarify the user's requirement by applying structured thinking frameworks. Generate targeted questions using the CTF formula and engineering mindset. Produce exactly ONE key question per response with 3-5 professionally crafted options.

**F (Format)**: Use structured Markdown format with clear sections, emoji indicators, and consistent organization. Separate instructions from content using ### markers.

### INPUT DATA
<requirement>
{message}
</requirement>

<conversation_history>
{history}
</conversation_history>

<search_context>
{search_info if search_info else "No search context available"}
</search_context>

### CORE ENGINEERING PRINCIPLES (Must Follow)

**Task 1: Foundation Engineering**
- Apply CTF (Context-Task-Format) formula to all interactions
- Use positive, action-oriented instructions (Do X, not "Don't do Y")
- Maintain structural separation between System directives and User content
- Use ### markers and XML-style tags for information isolation

**Task 2: Deep Reasoning Activation**
- Strategy Selection: Use Zero-Shot for simple questions, Few-Shot for format-specific guidance
- For logical/complex requirements: Apply Chain-of-Thought ("Let me analyze step by step")
- For ambiguous scenarios: Use Step-Back technique (first establish core principles, then specifics)
- Implement mandatory self-correction: Draft â†’ Identify gaps â†’ Refine

**Task 3: Response Control & Standardization**
- Enforce structured output with consistent formatting
- Maintain professional, objective tone throughout
- Place critical instructions at the end (combat "lost in the middle" effect)

**Task 4: Automated Workflow Architecture**
- Break complex clarification into logical phases
- Each interaction focuses on one specific dimension
- Build progressive understanding through structured questioning

### INTERACTION RULES

**Rule 1**: Ask ONLY ONE key question per response, focusing on a single clarification dimension
**Rule 2**: Provide 3-4 reference options covering different strategic directions
**Rule 3**: Options must be actionable, specific, and mutually exclusive where possible
**Rule 4**: If user says "Accept" (or similar confirmation), provide Requirement Summary AND Optimized Prompt
**Rule 5**: Questions must follow logical progression, diving deeper based on accumulated context
**Rule 6**: For complex technical requirements, apply CoT: "Let me think through this systematically..."

### RESPONSE STRUCTURES

**Normal Clarification Format:**
```
ğŸ” **Question**: [Apply CTF: single, precise question focusing on one dimension]

**Analysis Framework**: [Briefly state the thinking approach - Zero-Shot, CoT, Step-Back, etc.]

**Strategic Options**:
- [Option 1: Clear, actionable direction]
- [Option 2: Alternative approach or focus]
- [Option 3: Different methodology or scope]
- [Option 4: Complementary perspective]

ğŸ’¡ **Action**: Select one or more options, or provide specific details in your own words
```

**Final Acceptance Format:**
```
âœ… **Requirement Summary**:
[Apply systematic analysis - distill clarified requirements into coherent brief]

ğŸš€ **Optimized Prompt**:
[Professional-grade prompt applying CTF formula, ready for immediate use]

ğŸ“‹ **Implementation Notes**:
[Key considerations, parameters, or context for best results]
```

### EXECUTION PROTOCOL
Start systematic analysis now. Apply the appropriate reasoning strategy based on requirement complexity."""

        # ä¸€æ¬¡æ€§ç”Ÿæˆå›å¤
        full_response = await llm.ainvoke(prompt)

        # å‘é€å®Œæˆäº‹ä»¶
        await sio.emit('stream_complete', {
            'full_content': full_response.content,
            'conversation_id': conversation_id,
            'search_info': search_info
        }, room=sid)
        
        # ä¿å­˜å¯¹è¯
        await save_conversation(conversation_id, message, full_response.content)
        
    except Exception as e:
        print(f"Error in chat_message: {e}")
        await sio.emit('error', {'message': str(e)}, room=sid)

# REST API è·¯ç”±
@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "éœ€æ±‚æ¾„æ¸…åŠ©æ‰‹APIæœåŠ¡è¿è¡Œä¸­ (æ”¯æŒWebSocket)",
        "version": "1.0.0",
        "endpoints": {
            "chat": "/chat - å¯¹è¯æ¥å£ (REST)",
            "analyze": "/analyze - éœ€æ±‚åˆ†æ",
            "health": "/health - å¥åº·æ£€æŸ¥"
        }
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "services": {
            "llm": "connected" if os.getenv("DEEPSEEK_API_KEY") else "disconnected",
            "search": "enabled" if os.getenv("SERPER_API_KEY") else "disabled",
            "websocket": "enabled"
        }
    }

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest, background_tasks: BackgroundTasks):
    """å¯¹è¯æ¥å£ (RESTå…¼å®¹)"""
    try:
        conversation_id = generate_conversation_id()

        # è½¬æ¢å¯¹è¯å†å²
        history = [{"role": msg.role, "content": msg.content} for msg in request.conversation_history]

        # æœç´¢ä¿¡æ¯
        search_info = ""
        if (request.enable_search and
            len(history) == 0 and
            len(request.message) > 10 and
            os.getenv("SERPER_API_KEY")):
            try:
                search_info = search_requirement_context(request.message)
            except Exception as e:
                search_info = f"æœç´¢æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"

        # æ„å»ºæç¤ºè¯
        prompt = f"""You are a professional requirement clarification assistant. Help users clarify their needs through targeted questions, ultimately outputting an optimized prompt.

User requirement: {request.message}
Conversation history: {history}
{search_info if search_info else ""}

Follow these rules strictly:

**Rule 1:** Ask ONLY ONE key question at a time to help clarify specific needs
**Rule 2:** Provide 3-5 reference options after each question for users to choose from (they can select multiple or provide their own answer)
**Rule 3:** Options should cover different possible directions
**Rule 4:** If user says "Accept" (or similar confirmation), DO NOT ask more questions. Instead, output a "Requirement Summary" and the "Optimized Prompt".
**Rule 5:** Questions should be progressive, diving deeper based on user's answers

Response format (Normal):
```
ğŸ” **Question**: [Your question here]

**Options**:
- [Option 1 text]
- [Option 2 text]
- [Option 3 text]
- [Option 4 text]

ğŸ’¡ You can select one or more options above, or describe in your own words
```

Response format (When user says "Accept"):
```
âœ… **Requirement Summary**:
[Brief summary of the clarified requirements]

ğŸš€ **Optimized Prompt**:
[The final, detailed prompt that the user can use]
```

Start analysis:"""

        # è·å–AIå›å¤
        response = llm.invoke(prompt)

        # ä¿å­˜å¯¹è¯ï¼ˆå¼‚æ­¥ï¼‰
        background_tasks.add_task(
            save_conversation,
            conversation_id,
            request.message,
            response.content
        )

        return ChatResponse(
            response=response.content,
            timestamp=datetime.now().isoformat(),
            search_info=search_info if search_info else None,
            conversation_id=conversation_id
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/conversations/{conversation_id}")
async def get_conversation(conversation_id: str):
    """è·å–å¯¹è¯å†å²"""
    if conversation_id not in conversations:
        raise HTTPException(status_code=404, detail="å¯¹è¯ä¸å­˜åœ¨")

    return {
        "conversation_id": conversation_id,
        "messages": conversations[conversation_id]
    }

@app.post("/analyze", response_model=RequirementAnalysis)
async def analyze_requirement(request: ChatRequest):
    """éœ€æ±‚åˆ†ææ¥å£"""
    try:
        # æœç´¢ç›¸å…³ä¿¡æ¯
        search_info = ""
        if os.getenv("SERPER_API_KEY"):
            try:
                search_info = search_requirement_context(request.message)
            except Exception as e:
                search_info = f"æœç´¢æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"

        # æ„å»ºåˆ†ææç¤º
        prompt = f"""è¯·å¯¹ä»¥ä¸‹ç”¨æˆ·éœ€æ±‚è¿›è¡Œæ·±åº¦åˆ†æï¼Œç”Ÿæˆä¼˜åŒ–åçš„æç¤ºè¯ï¼š

ç”¨æˆ·åŸå§‹éœ€æ±‚ï¼š{request.message}

{search_info if search_info else ""}

åŸºäºç”¨æˆ·éœ€æ±‚ï¼Œç”Ÿæˆä¸€ä¸ªå®Œæ•´ã€æ¸…æ™°ã€å…·ä½“çš„ä¼˜åŒ–æç¤ºè¯ã€‚è¿™ä¸ªæç¤ºè¯åº”è¯¥ï¼š

1. **æ˜ç¡®ç›®æ ‡**ï¼šæ¸…æ¥šè¯´æ˜è¦è¾¾æˆçš„ç›®æ ‡
2. **å…·ä½“è¦æ±‚**ï¼šåˆ—å‡ºè¯¦ç»†çš„åŠŸèƒ½å’Œç‰¹æ€§è¦æ±‚
3. **æŠ€æœ¯è§„èŒƒ**ï¼šåŒ…å«æŠ€æœ¯æ ˆã€æ¶æ„ã€æ€§èƒ½è¦æ±‚ç­‰
4. **ç”¨æˆ·ä½“éªŒ**ï¼šæè¿°ç•Œé¢è®¾è®¡ã€äº¤äº’æµç¨‹ç­‰
5. **è¾¹ç•Œæ¡ä»¶**ï¼šæ˜ç¡®åŒ…å«å’Œä¸åŒ…å«çš„å†…å®¹

è¯·è¾“å‡ºä¸€ä¸ªå®Œæ•´çš„ä¼˜åŒ–æç¤ºè¯ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

```
## ä¼˜åŒ–æç¤ºè¯

**ç›®æ ‡**ï¼š[æ˜ç¡®çš„é¡¹ç›®ç›®æ ‡]

**æ ¸å¿ƒåŠŸèƒ½**ï¼š
- åŠŸèƒ½1ï¼š[è¯¦ç»†æè¿°]
- åŠŸèƒ½2ï¼š[è¯¦ç»†æè¿°]
- åŠŸèƒ½3ï¼š[è¯¦ç»†æè¿°]

**æŠ€æœ¯è¦æ±‚**ï¼š
- æŠ€æœ¯æ ˆï¼š[å…·ä½“æŠ€æœ¯è¦æ±‚]
- æ¶æ„ï¼š[æ¶æ„è®¾è®¡è¦æ±‚]
- æ€§èƒ½ï¼š[æ€§èƒ½æŒ‡æ ‡è¦æ±‚]

**ç”¨æˆ·ä½“éªŒ**ï¼š
- ç•Œé¢è®¾è®¡ï¼š[UI/UXè¦æ±‚]
- äº¤äº’æµç¨‹ï¼š[ç”¨æˆ·æ“ä½œæµç¨‹]
- å“åº”å¼ï¼š[è®¾å¤‡å…¼å®¹æ€§è¦æ±‚]

**å…¶ä»–è¦æ±‚**ï¼š
- [å…¶ä»–é‡è¦çº¦æŸå’Œæ¡ä»¶]
```

è¯·ç”Ÿæˆä¼˜åŒ–æç¤ºè¯ï¼š"""

        response = llm.invoke(prompt)

        # å°è¯•è§£æJSONå“åº”
        try:
            import json
            analysis_data = json.loads(response.content)
        except:
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œè¿”å›åŸºæœ¬åˆ†æ
            analysis_data = {
                "optimized_requirement": request.message,
                "key_questions": [],
                "suggestions": ["éœ€æ±‚åˆ†æå®Œæˆ"]
            }

        return RequirementAnalysis(
            original_requirement=request.message,
            optimized_requirement=analysis_data.get("optimized_requirement", request.message),
            key_questions=analysis_data.get("key_questions", []),
            suggestions=analysis_data.get("suggestions", [])
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/stats")
async def get_stats():
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    total_conversations = len(conversations)
    total_messages = sum(len(msgs) for msgs in conversations.values())

    return {
        "total_conversations": total_conversations,
        "total_messages": total_messages,
        "average_messages_per_conversation": total_messages / total_conversations if total_conversations > 0 else 0,
        "active_conversations": len([conv for conv in conversations.values() if len(conv) > 0])
    }

if __name__ == "__main__":
    import uvicorn

    # å¯åŠ¨APIæœåŠ¡å™¨
    uvicorn.run(
        "server:socket_app",  # ä½¿ç”¨socket_appè€Œä¸æ˜¯app
        host="127.0.0.1",
        port=8000,
        reload=True,
        log_level="info"
    )